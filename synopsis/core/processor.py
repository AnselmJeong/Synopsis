"""
EPUB íŒŒì¼ ë¶„ì„ ë° PostgreSQL ì €ì¥ ì‹œìŠ¤í…œ
ê³„ì¸µì  TOC êµ¬ì¡°ë¥¼ ì¶”ì¶œí•˜ê³  Gemini ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import json
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import psycopg
from pgvector.psycopg import register_vector
import google.generativeai as genai
from tqdm import tqdm
from dotenv import load_dotenv

from ..models.toc import TOCEntry
from ..models.chunk import ContentChunk
from .database import setup_database

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class EPUBProcessor:
    """EPUB íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        gemini_api_key: Optional[str] = None,
        connection_string: str = "postgresql://user@localhost:5432/postgres",
        target_chunk_size: int = 4096,
        min_chunk_size: int = 1000,
        max_chunk_size: int = 6000,
    ):
        """
        EPUB í”„ë¡œì„¸ì„œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            gemini_api_key: Gemini API í‚¤
            connection_string: PostgreSQL ì—°ê²° ë¬¸ìì—´
            target_chunk_size: ëª©í‘œ ì²­í¬ í¬ê¸° (ë¬¸ì ë‹¨ìœ„)
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
        """
        self.gemini_api_key = gemini_api_key or os.getenv("GEMINI_API_KEY")
        self.connection_string = connection_string
        self.target_chunk_size = target_chunk_size
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size

        logger.info(
            f"EPUB í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”: ëª©í‘œ={target_chunk_size}, ìµœì†Œ={min_chunk_size}, ìµœëŒ€={max_chunk_size}"
        )

        if not self.gemini_api_key:
            logger.warning(
                "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ë² ë”© ìƒì„±ì´ ê±´ë„ˆë›°ì–´ì§‘ë‹ˆë‹¤."
            )
        else:
            genai.configure(api_key=self.gemini_api_key)
            self.model = genai.GenerativeModel("gemini-pro")

    def extract_toc_hierarchy(self, epub_file_path: str) -> List[TOCEntry]:
        """
        EPUB íŒŒì¼ì—ì„œ ê³„ì¸µì  TOCë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            epub_file_path: EPUB íŒŒì¼ ê²½ë¡œ

        Returns:
            TOC í•­ëª©ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"EPUB íŒŒì¼ì—ì„œ TOCë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘: {epub_file_path}")

        try:
            book = epub.read_epub(epub_file_path)
            toc_entries = []
            current_hierarchy = []

            def process_toc_item(item, level: int = 0):
                """TOC í•­ëª©ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
                nonlocal current_hierarchy, toc_entries

                if isinstance(item, tuple):
                    # (Section, [NavPoint, ...]) í˜•íƒœ
                    section, nav_points = item
                    if hasattr(section, "title"):
                        title = section.title
                    else:
                        title = str(section)

                    # í˜„ì¬ ê³„ì¸µì— ì¶”ê°€
                    hierarchy_entry = {"level": level, "title": title}
                    current_hierarchy = current_hierarchy[:level] + [hierarchy_entry]

                    # NavPoint ì²˜ë¦¬
                    for nav_point in nav_points:
                        process_toc_item(nav_point, level + 1)

                elif hasattr(item, "title") and hasattr(item, "href"):
                    # NavPoint ê°ì²´
                    title = item.title.strip()
                    href = item.href

                    # íŒŒì¼ ê²½ë¡œì™€ ì•µì»¤ ë¶„ë¦¬
                    if "#" in href:
                        file_path, anchor = href.split("#", 1)
                    else:
                        file_path, anchor = href, None

                    # í˜„ì¬ ê³„ì¸µì— ì¶”ê°€
                    hierarchy_entry = {"level": level, "title": title}
                    current_hierarchy = current_hierarchy[:level] + [hierarchy_entry]

                    # TOC ì—”íŠ¸ë¦¬ ìƒì„±
                    toc_entry = TOCEntry(
                        title=title,
                        level=level,
                        file_path=file_path,
                        hierarchy=list(current_hierarchy),
                        anchor=anchor,
                    )
                    toc_entries.append(toc_entry)

                    logger.debug(f"TOC í•­ëª© ì¶”ê°€: {title} (ë ˆë²¨ {level})")

            # TOC ì²˜ë¦¬
            for toc_item in book.toc:
                process_toc_item(toc_item)

            logger.info(f"ì´ {len(toc_entries)}ê°œì˜ TOC í•­ëª©ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
            return toc_entries

        except Exception as e:
            logger.error(f"TOC ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def extract_content_from_file(self, book: epub.EpubBook, file_path: str) -> str:
        """
        EPUBì—ì„œ íŠ¹ì • íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê°„ë‹¨í•œ ë²„ì „)

        Args:
            book: EPUB ì±… ê°ì²´
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
        """
        logger.debug(f"íŒŒì¼ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘: {file_path}")

        try:
            # íŒŒì¼ ê²½ë¡œ ì •ê·œí™”
            if file_path.startswith("/"):
                file_path = file_path[1:]

            # ì•„ì´í…œ ì°¾ê¸°
            item = None
            for book_item in book.get_items():
                if book_item.get_type() == ebooklib.ITEM_DOCUMENT:
                    if (
                        book_item.get_name() == file_path
                        or book_item.file_name == file_path
                        or book_item.get_name().endswith(file_path)
                    ):
                        item = book_item
                        break

            if not item:
                logger.warning(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return ""

            # HTML ì½˜í…ì¸  íŒŒì‹±
            html_content = item.get_content().decode("utf-8", errors="ignore")
            soup = BeautifulSoup(html_content, "html.parser")

            # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.decompose()

            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            text = soup.get_text()

            # í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = " ".join(chunk for chunk in chunks if chunk)

            logger.debug(f"ì¶”ì¶œëœ ì½˜í…ì¸  ê¸¸ì´: {len(text)}ì")
            return text

        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def create_smart_chunks(
        self, content: str, toc_entry: TOCEntry
    ) -> List[ContentChunk]:
        """
        ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: ìì—°ìŠ¤ëŸ¬ìš´ ì†Œì œëª©ì„ ê°ì§€í•˜ì—¬ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            content: ì²­í‚¹í•  ì½˜í…ì¸ 
            toc_entry: TOC í•­ëª©

        Returns:
            ìƒì„±ëœ ì²­í¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‹œì‘: {toc_entry.title}")

        if not content.strip():
            logger.warning("ë¹ˆ ì½˜í…ì¸ ì…ë‹ˆë‹¤.")
            return []

        try:
            # 1. ìì—°ìŠ¤ëŸ¬ìš´ ì†Œì œëª© ê°ì§€
            sections = self._detect_natural_subtitles(content)
            logger.info(f"{len(sections)}ê°œì˜ ì„¹ì…˜ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")

            # 2. ê° ì„¹ì…˜ì„ ì²­í¬ë¡œ ë³€í™˜
            all_chunks = []
            for i, section in enumerate(sections):
                section_chunks = self._create_chunks_from_section(
                    section, toc_entry, len(all_chunks)
                )
                all_chunks.extend(section_chunks)

            # 3. í¬ê¸° ê²€ì¦ ë° ê°•ì œ ë¶„í• 
            validated_chunks = []
            for chunk in all_chunks:
                if len(chunk.content) <= self.max_chunk_size:
                    validated_chunks.append(chunk)
                else:
                    logger.warning(
                        f"ì²­í¬ í¬ê¸° ì´ˆê³¼ ({len(chunk.content)}ì), ê°•ì œ ë¶„í•  ì‹¤í–‰"
                    )
                    split_contents = self._force_split_content(chunk.content)
                    for j, split_content in enumerate(split_contents):
                        new_chunk = ContentChunk(
                            content=split_content,
                            toc_entry=chunk.toc_entry,
                            chunk_index=len(validated_chunks),
                            total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                            section_info={
                                **chunk.section_info,
                                "force_split": True,
                                "force_split_part": j + 1,
                                "force_split_total": len(split_contents),
                            },
                        )
                        validated_chunks.append(new_chunk)

            # 4. total_chunks ì—…ë°ì´íŠ¸
            total_chunks = len(validated_chunks)
            for chunk in validated_chunks:
                chunk.total_chunks = total_chunks

            logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì™„ë£Œ: {total_chunks}ê°œ ì²­í¬ ìƒì„±")
            return validated_chunks

        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ì²­í‚¹
            logger.info("ê°„ë‹¨í•œ ì²­í‚¹ìœ¼ë¡œ í´ë°±")
            return self._create_simple_chunks_fallback(content, toc_entry)

    def _detect_natural_subtitles(self, content: str) -> List[Dict[str, Any]]:
        """
        ìì—°ìŠ¤ëŸ¬ìš´ ì†Œì œëª©ì„ ê°ì§€í•˜ì—¬ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

        Args:
            content: ë¶„ì„í•  ì½˜í…ì¸ 

        Returns:
            ì„¹ì…˜ ì •ë³´ì˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.debug("ìì—°ìŠ¤ëŸ¬ìš´ ì†Œì œëª© ê°ì§€ ì¤‘...")

        paragraphs = content.split("\n\n")
        paragraphs = [p.strip() for p in paragraphs if p.strip()]

        if not paragraphs:
            return [{"title": "ì „ì²´ ì½˜í…ì¸ ", "content": content, "start_idx": 0}]

        sections = []
        current_section_start = 0
        current_section_title = "ì„œë¡ "

        for i, paragraph in enumerate(paragraphs):
            if self._is_natural_subtitle(paragraph, i, paragraphs):
                # ì´ì „ ì„¹ì…˜ ë§ˆë¬´ë¦¬
                if i > current_section_start:
                    section_content = "\n\n".join(paragraphs[current_section_start:i])
                    sections.append(
                        {
                            "title": current_section_title,
                            "content": section_content,
                            "start_idx": current_section_start,
                            "end_idx": i - 1,
                        }
                    )

                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section_title = paragraph
                current_section_start = i + 1

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_section_start < len(paragraphs):
            section_content = "\n\n".join(paragraphs[current_section_start:])
            sections.append(
                {
                    "title": current_section_title,
                    "content": section_content,
                    "start_idx": current_section_start,
                    "end_idx": len(paragraphs) - 1,
                }
            )

        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
        if not sections:
            sections = [{"title": "ì „ì²´ ì½˜í…ì¸ ", "content": content, "start_idx": 0}]

        logger.debug(f"{len(sections)}ê°œì˜ ì„¹ì…˜ ê°ì§€ ì™„ë£Œ")
        return sections

    def _is_natural_subtitle(
        self, paragraph: str, index: int, all_paragraphs: List[str]
    ) -> bool:
        """
        ë¬¸ë‹¨ì´ ìì—°ìŠ¤ëŸ¬ìš´ ì†Œì œëª©ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.

        Args:
            paragraph: ê²€ì‚¬í•  ë¬¸ë‹¨
            index: ë¬¸ë‹¨ì˜ ì¸ë±ìŠ¤
            all_paragraphs: ì „ì²´ ë¬¸ë‹¨ ëª©ë¡

        Returns:
            ì†Œì œëª© ì—¬ë¶€
        """
        # 1. ê¸°ë³¸ ê¸¸ì´ ì²´í¬
        if len(paragraph) > 80:
            return False

        # 2. ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´ (ì˜ˆ: "1. ì„œë¡ ", "2.3 ì–‘ìì—­í•™")
        if re.match(r"^\d+[\.\)]\s+", paragraph) or re.match(
            r"^\d+\.\d+[\.\)]\s+", paragraph
        ):
            return True

        # 3. íŠ¹ì • í‚¤ì›Œë“œë¡œ ì‹œì‘
        subtitle_keywords = [
            "ê°€ì¥",
            "ìƒˆë¡œìš´",
            "ì²« ë²ˆì§¸",
            "ë‘ ë²ˆì§¸",
            "ì„¸ ë²ˆì§¸",
            "ë§ˆì§€ë§‰",
            "ê²°ë¡ ",
            "ì„œë¡ ",
            "ê°œìš”",
            "ìš”ì•½",
            "The",
            "New",
            "First",
            "Second",
            "Third",
            "Last",
            "Conclusion",
            "Introduction",
            "Overview",
            "Summary",
        ]

        for keyword in subtitle_keywords:
            if paragraph.startswith(keyword):
                return True

        # 4. ê¸¸ì´ì™€ êµ¬ì¡°ì  íŠ¹ì§•
        if len(paragraph) < 80:
            # ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
            if paragraph.endswith(":"):
                return True

            # ë‹¤ìŒ ë¬¸ë‹¨ì´ í˜„ì¬ë³´ë‹¤ ìƒë‹¹íˆ ê¸´ ê²½ìš°
            if index + 1 < len(all_paragraphs):
                next_paragraph = all_paragraphs[index + 1]
                if len(next_paragraph) > len(paragraph) * 3:
                    return True

        return False

    def _create_chunks_from_section(
        self, section: Dict[str, Any], toc_entry: TOCEntry, start_index: int
    ) -> List[ContentChunk]:
        """
        ì„¹ì…˜ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            section: ì„¹ì…˜ ì •ë³´
            toc_entry: TOC í•­ëª©
            start_index: ì‹œì‘ ì²­í¬ ì¸ë±ìŠ¤

        Returns:
            ìƒì„±ëœ ì²­í¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        content = section["content"]
        section_title = section["title"]

        if not content.strip():
            return []

        chunks = []

        if len(content) <= self.target_chunk_size:
            # ë‹¨ì¼ ì²­í¬ë¡œ ì¶©ë¶„
            chunk = ContentChunk(
                content=content,
                toc_entry=toc_entry,
                chunk_index=start_index,
                total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                section_info={
                    "section_title": section_title,
                    "section_index": section.get("start_idx", 0),
                    "is_single_chunk": True,
                },
            )
            chunks.append(chunk)
        else:
            # ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
            content_parts = self._split_content_preserving_paragraphs(content)

            for i, part in enumerate(content_parts):
                chunk = ContentChunk(
                    content=part,
                    toc_entry=toc_entry,
                    chunk_index=start_index + i,
                    total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                    section_info={
                        "section_title": section_title,
                        "section_index": section.get("start_idx", 0),
                        "chunk_part": i + 1,
                        "chunk_total_parts": len(content_parts),
                    },
                )
                chunks.append(chunk)

        logger.debug(f"ì„¹ì…˜ '{section_title}'ì—ì„œ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks

    def _split_content_preserving_paragraphs(self, content: str) -> List[str]:
        """
        ë¬¸ë‹¨ ê²½ê³„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì½˜í…ì¸ ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            content: ë¶„í• í•  ì½˜í…ì¸ 

        Returns:
            ë¶„í• ëœ ì½˜í…ì¸  ë¶€ë¶„ë“¤
        """
        paragraphs = content.split("\n\n")
        paragraphs = [p.strip() for p in paragraphs if p.strip()]

        if not paragraphs:
            return []

        parts = []
        current_part = ""

        for paragraph in paragraphs:
            # ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° ì²´í¬
            test_content = (
                current_part + "\n\n" + paragraph if current_part else paragraph
            )

            if len(test_content) <= self.target_chunk_size:
                current_part = test_content
            else:
                # í˜„ì¬ íŒŒíŠ¸ ì €ì¥
                if current_part:
                    parts.append(current_part)

                # ìƒˆ íŒŒíŠ¸ ì‹œì‘
                current_part = paragraph

        # ë§ˆì§€ë§‰ íŒŒíŠ¸ ì €ì¥
        if current_part:
            parts.append(current_part)

        return parts

    def _split_by_sentences(self, text: str) -> List[str]:
        """
        ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸

        Returns:
            ë¶„í• ëœ ë¬¸ì¥ë“¤
        """
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ë¶„ë¦¬ íŒ¨í„´
        sentence_endings = r"[.!?ã€‚ï¼ï¼Ÿ]\s+"
        sentences = re.split(sentence_endings, text.strip())

        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]

        return sentences

    def _create_simple_chunks_fallback(
        self, content: str, toc_entry: TOCEntry
    ) -> List[ContentChunk]:
        """
        ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ í´ë°± ì²­í‚¹

        Args:
            content: ì²­í‚¹í•  ì½˜í…ì¸ 
            toc_entry: TOC í•­ëª©

        Returns:
            ìƒì„±ëœ ì²­í¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ê°„ë‹¨í•œ í´ë°± ì²­í‚¹ ì‹¤í–‰")

        if not content.strip():
            return []

        chunks = []
        remaining_content = content

        chunk_index = 0
        while remaining_content:
            # ëª©í‘œ í¬ê¸°ë§Œí¼ ìë¥´ê¸°
            if len(remaining_content) <= self.target_chunk_size:
                chunk_content = remaining_content
                remaining_content = ""
            else:
                # ë¬¸ë‹¨ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
                paragraphs = remaining_content.split("\n\n")
                chunk_content = ""
                used_paragraphs = 0

                for i, paragraph in enumerate(paragraphs):
                    test_content = (
                        chunk_content + "\n\n" + paragraph
                        if chunk_content
                        else paragraph
                    )
                    if len(test_content) <= self.target_chunk_size:
                        chunk_content = test_content
                        used_paragraphs = i + 1
                    else:
                        break

                if used_paragraphs == 0:
                    # ì²« ë²ˆì§¸ ë¬¸ë‹¨ë„ ë„ˆë¬´ í° ê²½ìš°, ê°•ì œë¡œ ìë¦„
                    chunk_content = remaining_content[: self.target_chunk_size]
                    # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
                    last_space = chunk_content.rfind(" ")
                    if last_space > self.target_chunk_size // 2:
                        chunk_content = chunk_content[:last_space]

                    remaining_content = remaining_content[len(chunk_content) :].strip()
                else:
                    # ì‚¬ìš©ëœ ë¬¸ë‹¨ë“¤ ì œê±°
                    remaining_paragraphs = paragraphs[used_paragraphs:]
                    remaining_content = "\n\n".join(remaining_paragraphs).strip()

            # ì²­í¬ ìƒì„±
            if chunk_content.strip():
                chunk = ContentChunk(
                    content=chunk_content.strip(),
                    toc_entry=toc_entry,
                    chunk_index=chunk_index,
                    total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                    section_info={
                        "fallback_chunking": True,
                        "chunk_method": "simple",
                    },
                )
                chunks.append(chunk)
                chunk_index += 1

        # total_chunks ì—…ë°ì´íŠ¸
        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        logger.info(f"í´ë°± ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks

    def _split_paragraph_to_chunks(
        self, paragraph: str, toc_entry: TOCEntry, start_index: int
    ) -> List[ContentChunk]:
        """
        ë‹¨ì¼ ë¬¸ë‹¨ì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            paragraph: ë¶„í• í•  ë¬¸ë‹¨
            toc_entry: TOC í•­ëª©
            start_index: ì‹œì‘ ì²­í¬ ì¸ë±ìŠ¤

        Returns:
            ìƒì„±ëœ ì²­í¬ë“¤
        """
        if len(paragraph) <= self.target_chunk_size:
            chunk = ContentChunk(
                content=paragraph,
                toc_entry=toc_entry,
                chunk_index=start_index,
                total_chunks=1,
                section_info={"single_paragraph": True},
            )
            return [chunk]

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_by_sentences(paragraph)
        chunks = []
        current_chunk = ""
        chunk_index = start_index

        for sentence in sentences:
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence

            if len(test_chunk) <= self.target_chunk_size:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk = ContentChunk(
                        content=current_chunk.strip(),
                        toc_entry=toc_entry,
                        chunk_index=chunk_index,
                        total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                        section_info={
                            "paragraph_split": True,
                            "sentence_based": True,
                        },
                    )
                    chunks.append(chunk)
                    chunk_index += 1

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = sentence

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunk = ContentChunk(
                content=current_chunk.strip(),
                toc_entry=toc_entry,
                chunk_index=chunk_index,
                total_chunks=0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                section_info={
                    "paragraph_split": True,
                    "sentence_based": True,
                },
            )
            chunks.append(chunk)

        # total_chunks ì—…ë°ì´íŠ¸
        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        return chunks

    def _split_by_words(self, text: str, max_size: int) -> List[str]:
        """
        ë‹¨ì–´ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            max_size: ìµœëŒ€ í¬ê¸°

        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ë¶€ë¶„ë“¤
        """
        if len(text) <= max_size:
            return [text]

        words = text.split()
        parts = []
        current_part = ""

        for word in words:
            test_part = current_part + " " + word if current_part else word

            if len(test_part) <= max_size:
                current_part = test_part
            else:
                if current_part:
                    parts.append(current_part)
                current_part = word

        if current_part:
            parts.append(current_part)

        return parts

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            text: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        if not self.gemini_api_key:
            return None

        try:
            result = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document",
            )
            return result["embedding"]
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def save_to_database(
        self, chunks: List[ContentChunk], book_title: str, book_author: str
    ):
        """
        ì²­í¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            chunks: ì €ì¥í•  ì²­í¬ë“¤
            book_title: ì±… ì œëª©
            book_author: ì €ì
        """
        if not chunks:
            logger.warning("ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            conn = psycopg.connect(self.connection_string)
            register_vector(conn)

            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'documents'
                    );
                """)
                table_exists = cursor.fetchone()[0]

                if not table_exists:
                    logger.info(
                        "documents í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."
                    )
                    conn.close()
                    setup_database(self.connection_string)
                    conn = psycopg.connect(self.connection_string)
                    register_vector(conn)

            logger.info(f"{len(chunks)}ê°œì˜ ì²­í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...")

            with conn.cursor() as cursor:
                # ì²­í¬ë³„ë¡œ ì²˜ë¦¬
                for chunk in tqdm(chunks, desc="ì²­í¬ ì €ì¥"):
                    # ì„ë² ë”© ìƒì„±
                    embedding = self.generate_embedding(chunk.content)

                    # ê³„ì¸µ êµ¬ì¡° ìƒì„±
                    hierarchy = {
                        "toc_hierarchy": chunk.toc_entry.hierarchy,
                        "toc_title": chunk.toc_entry.title,
                        "toc_level": chunk.toc_entry.level,
                        "file_path": chunk.toc_entry.file_path,
                    }

                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = {
                        "chunk_index": chunk.chunk_index,
                        "total_chunks": chunk.total_chunks,
                        "content_length": len(chunk.content),
                        "section_info": chunk.section_info or {},
                    }

                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
                    insert_query = """
                    INSERT INTO documents (content, embedding, hierarchy, metadata, book_title, book_author)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    """

                    cursor.execute(
                        insert_query,
                        (
                            chunk.content,
                            embedding,
                            json.dumps(hierarchy, ensure_ascii=False),
                            json.dumps(metadata, ensure_ascii=False),
                            book_title,
                            book_author,
                        ),
                    )

                # ë³€ê²½ì‚¬í•­ ì»¤ë°‹
                conn.commit()

            logger.info("ëª¨ë“  ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if conn:
                conn.close()

    def process_epub(self, epub_file_path: str, save_toc_json: bool = True):
        """
        EPUB íŒŒì¼ì„ ì „ì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            epub_file_path: EPUB íŒŒì¼ ê²½ë¡œ
            save_toc_json: TOC JSON íŒŒì¼ ì €ì¥ ì—¬ë¶€
        """
        logger.info(f"EPUB íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {epub_file_path}")

        try:
            # 1. EPUB íŒŒì¼ ì½ê¸° ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            book = epub.read_epub(epub_file_path)

            # ì±… ì œëª©ê³¼ ì €ì ì¶”ì¶œ
            book_title = None
            book_author = None

            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª©ê³¼ ì €ì ì¶”ì¶œ
            if hasattr(book, "metadata") and book.metadata:
                for namespace, metadata_items in book.metadata.items():
                    for key, item_list in metadata_items.items():
                        for item in item_list:
                            # itemì€ (text, attributes) íŠœí”Œ í˜•íƒœ
                            if isinstance(item, tuple) and len(item) >= 1:
                                text_content = item[0]
                                if key.lower() == "title" and text_content:
                                    book_title = text_content
                                    print(f"    ğŸ“– ì±… ì œëª© ë°œê²¬: {book_title}")
                                elif (
                                    key.lower() in ["creator", "author"]
                                    and text_content
                                ):
                                    book_author = text_content
                                    print(f"    âœï¸ ì €ì ë°œê²¬: {book_author}")

            # ì±… ì œëª©ì´ ì—†ëŠ” ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
            if not book_title:
                book_title = Path(epub_file_path).stem
                logger.warning(
                    f"ë©”íƒ€ë°ì´í„°ì—ì„œ ì±… ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íŒŒì¼ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {book_title}"
                )

            print(f"ğŸ“– ìµœì¢… ì±… ì œëª©: {book_title}")
            if book_author:
                print(f"âœï¸ ìµœì¢… ì €ì: {book_author}")

            # 2. TOC ì¶”ì¶œ
            toc_entries = self.extract_toc_hierarchy(epub_file_path)

            if save_toc_json:
                # TOCë¥¼ JSONìœ¼ë¡œ ì €ì¥
                toc_json_path = Path(epub_file_path).parent / (
                    Path(epub_file_path).stem + "_toc.json"
                )
                toc_data = []

                for entry in toc_entries:
                    toc_data.append(
                        {
                            "title": entry.title,
                            "level": entry.level,
                            "file_path": entry.file_path,
                            "hierarchy": entry.hierarchy,
                            "anchor": entry.anchor,
                        }
                    )

                with open(toc_json_path, "w", encoding="utf-8") as f:
                    json.dump(toc_data, f, ensure_ascii=False, indent=2)

                logger.info(f"TOC JSON íŒŒì¼ ì €ì¥: {toc_json_path}")

            # 3. ê° TOC í•­ëª©ë³„ë¡œ ì½˜í…ì¸  ì¶”ì¶œ ë° ì²­í‚¹
            all_chunks = []

            for toc_entry in toc_entries:
                logger.info(
                    f"TOC í•­ëª© ì²˜ë¦¬ ì¤‘: {toc_entry.title} (ë ˆë²¨ {toc_entry.level})"
                )

                # ì½˜í…ì¸  ì¶”ì¶œ
                content = self.extract_content_from_file(book, toc_entry.file_path)

                if content.strip():
                    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
                    chunks = self.create_smart_chunks(content, toc_entry)
                    all_chunks.extend(chunks)

                    logger.info(f"  -> {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ì´ {len(content)}ì)")
                else:
                    logger.warning(f"  -> ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {toc_entry.file_path}")

            logger.info(f"ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # í†µê³„ ì¶œë ¥
            if all_chunks:
                chunk_sizes = [len(chunk.content) for chunk in all_chunks]
                avg_size = sum(chunk_sizes) / len(chunk_sizes)
                min_size = min(chunk_sizes)
                max_size = max(chunk_sizes)

                logger.info(f"ì²­í¬ í¬ê¸° í†µê³„:")
                logger.info(f"  - í‰ê· : {avg_size:.0f}ì")
                logger.info(f"  - ìµœì†Œ: {min_size:,}ì")
                logger.info(f"  - ìµœëŒ€: {max_size:,}ì")
                logger.info(
                    f"  - ëª©í‘œ ëŒ€ë¹„ í‰ê· : {avg_size / self.target_chunk_size * 100:.1f}%"
                )

            # 4. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì±… ì œëª©ê³¼ ì €ì ì •ë³´ í¬í•¨)
            if all_chunks:
                self.save_to_database(
                    all_chunks, book_title=book_title, book_author=book_author
                )
            else:
                logger.warning(
                    "ìƒì„±ëœ ì²­í¬ê°€ ì—†ì–´ì„œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
                )

            logger.info("EPUB íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

        except Exception as e:
            logger.error(f"EPUB íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _force_split_content(self, content: str) -> List[str]:
        """
        í¬ê¸°ê°€ í° ì½˜í…ì¸ ë¥¼ ê°•ì œë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        ë¬¸ì¥ ê²½ê³„ â†’ ë¬¸ë‹¨ ê²½ê³„ â†’ ë‹¨ì–´ ê²½ê³„ â†’ ê°•ì œ ì ˆë‹¨ ìˆœì„œë¡œ ì‹œë„í•©ë‹ˆë‹¤.
        """
        if len(content) <= self.max_chunk_size:
            return [content]

        parts = []
        remaining = content
        safety_margin = 200  # ì•ˆì „ ì—¬ìœ ë¶„

        while remaining:
            if len(remaining) <= self.max_chunk_size:
                parts.append(remaining)
                break

            # ëª©í‘œ í¬ê¸° (ì—¬ìœ ë¶„ ê³ ë ¤)
            target_size = self.max_chunk_size - safety_margin

            # 1ë‹¨ê³„: ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            cut_pos = None
            sentence_endings = [".", "!", "?", "ã€‚", "ï¼", "ï¼Ÿ"]

            for i in range(target_size, max(target_size // 2, 500), -1):
                if i < len(remaining):
                    char = remaining[i]
                    # ë¬¸ì¥ ë¶€í˜¸ + ë‹¤ìŒ ë¬¸ìê°€ ê³µë°±ì¸ ê²½ìš°ë§Œ
                    if (
                        char in sentence_endings
                        and i + 1 < len(remaining)
                        and remaining[i + 1] in [" ", "\n", "\t"]
                    ):
                        cut_pos = i + 1
                        break

            # 2ë‹¨ê³„: ë¬¸ë‹¨ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if cut_pos is None:
                paragraph_breaks = ["\n\n", "\n"]
                for break_pattern in paragraph_breaks:
                    pos = remaining.rfind(break_pattern, 0, target_size)
                    if pos > target_size // 2:  # ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ
                        cut_pos = pos + len(break_pattern)
                        break

            # 3ë‹¨ê³„: ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if cut_pos is None:
                for i in range(target_size, max(target_size // 2, 500), -1):
                    if i < len(remaining):
                        char = remaining[i]
                        # ê³µë°±ì´ê±°ë‚˜ í•œê¸€/ì•ŒíŒŒë²³ì´ ëë‚˜ëŠ” ì§€ì 
                        if char in [" ", "\n", "\t"]:
                            cut_pos = i
                            break
                        elif i > 0:
                            prev_char = remaining[i - 1]
                            # í•œê¸€ì—ì„œ í•œê¸€ì´ ì•„ë‹Œ ë¬¸ìë¡œ ë³€í•˜ëŠ” ì§€ì 
                            if ord("ê°€") <= ord(prev_char) <= ord("í£") and not (
                                ord("ê°€") <= ord(char) <= ord("í£")
                            ):
                                cut_pos = i
                                break
                            # ì•ŒíŒŒë²³ì—ì„œ ì•ŒíŒŒë²³ì´ ì•„ë‹Œ ë¬¸ìë¡œ ë³€í•˜ëŠ” ì§€ì 
                            elif prev_char.isalpha() and not char.isalpha():
                                cut_pos = i
                                break

            # 4ë‹¨ê³„: ê°•ì œ ì ˆë‹¨ (ìµœì†Œ í¬ê¸° ë³´ì¥)
            if cut_pos is None:
                cut_pos = max(target_size, 1000)  # ìµœì†Œ 1000ìëŠ” ë³´ì¥

            # ë¶„í•  ì‹¤í–‰
            current_part = remaining[:cut_pos].strip()
            remaining = remaining[cut_pos:].strip()

            if current_part:
                parts.append(current_part)

            # ë¬´í•œë£¨í”„ ë°©ì§€
            if len(remaining) >= len(content):
                logger.error("ê°•ì œ ë¶„í• ì—ì„œ ë¬´í•œë£¨í”„ ê°ì§€")
                break

        return parts
